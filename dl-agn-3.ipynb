{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11856421,"sourceType":"datasetVersion","datasetId":7449976}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet wandb","metadata":{"_uuid":"8c2537de-5bb9-426f-9439-d654720524da","_cell_guid":"f976614a-b62d-42b9-abdf-601f0db5e5e8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom tqdm import tqdm\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport wandb\nfrom tqdm import tqdm\nimport numpy as np\nimport heapq\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport wandb\n","metadata":{"_uuid":"c1933df2-b55d-4569-8b2c-726a9b31863a","_cell_guid":"6bac942e-411f-4936-ad1f-32b940c53482","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-20T17:51:04.595598Z","iopub.execute_input":"2025-05-20T17:51:04.596122Z","iopub.status.idle":"2025-05-20T17:51:12.135746Z","shell.execute_reply.started":"2025-05-20T17:51:04.596101Z","shell.execute_reply":"2025-05-20T17:51:12.134986Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=secret_value_0)","metadata":{"_uuid":"b4baa3f4-7359-46e2-9a03-458cfb3dda38","_cell_guid":"66d76764-5c22-470a-b565-8afaee6e6242","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-20T17:51:12.136845Z","iopub.execute_input":"2025-05-20T17:51:12.137159Z","iopub.status.idle":"2025-05-20T17:51:18.391888Z","shell.execute_reply.started":"2025-05-20T17:51:12.137142Z","shell.execute_reply":"2025-05-20T17:51:18.391357Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m013\u001b[0m (\u001b[33mda24m013-iit-madras-alumni-association\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def load_data(file_path):\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split('\\t')\n            if len(parts) >= 2:\n                roman, native = parts[0], parts[1]\n                data.append((roman, native))\n    return data\n\n\ntrain_data = load_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv')\ndev_data = load_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv')\ntest_data = load_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv')","metadata":{"_uuid":"3c494dc9-3c36-4753-bf04-a710faedc432","_cell_guid":"f66b3be0-44d4-487b-b0b0-7917725f7232","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{"_uuid":"5ac71fae-10a6-4f32-ae03-4ae8f5cc37c1","_cell_guid":"24429610-3305-4dab-b675-12dc078fa607","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Convert loaded data into DataFrames\ntrain_df = pd.DataFrame(train_data, columns=['roman', 'native'])\ndev_df = pd.DataFrame(dev_data, columns=['roman', 'native'])\ntest_df = pd.DataFrame(test_data, columns=['roman', 'native'])\n\n# Quick look at the data\nprint(\"Train Set Sample:\")\nprint(train_df.head())\n\nprint(\"\\nDev Set Sample:\")\nprint(dev_df.head())\n\nprint(\"\\nTest Set Sample:\")\nprint(test_df.head())","metadata":{"_uuid":"03042590-60b0-42a1-bd99-1290493e1c6d","_cell_guid":"be761ae2-0585-4b4b-8f29-93bdb4679760","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check dataset sizes\nprint(\"\\nDataset sizes:\")\nprint(f\"Train: {len(train_df)} | Dev: {len(dev_df)} | Test: {len(test_df)}\")\n\n# Check for missing values\nprint(\"\\nMissing values:\")\nprint(train_df.isnull().sum())","metadata":{"_uuid":"a92c0183-ad9f-41ce-ba1b-b947868331ad","_cell_guid":"de795d4e-76d3-4726-a5a7-5385941c9114","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Check distribution of word lengths in roman and native\ntrain_df['roman_len'] = train_df['roman'].apply(len)\ntrain_df['native_len'] = train_df['native'].apply(len)\n\n# Plot histograms of word lengths\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(train_df['roman_len'], bins=20, kde=True, color='skyblue')\nplt.title('Length of Romanized Words (Train Set)')\nplt.xlabel('Number of characters')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.histplot(train_df['native_len'], bins=20, kde=True, color='salmon')\nplt.title('Length of Native Script Words (Train Set)')\nplt.xlabel('Number of characters')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"83768efe-a5b0-4bb0-9013-ecf4b1bf26e0","_cell_guid":"d9ca0c61-154a-4cf5-bc30-854cc9cdecc7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique roman and native characters\nunique_roman_chars = set(''.join(train_df['roman']))\nunique_native_chars = set(''.join(train_df['native']))\n\nprint(f\"\\nUnique Roman characters in train set: {len(unique_roman_chars)} → {sorted(unique_roman_chars)}\")\nprint(f\"Unique Native (Telugu) characters in train set: {len(unique_native_chars)} → {sorted(unique_native_chars)}\")","metadata":{"_uuid":"a7132ede-e47d-4e0e-bafa-2f27d9755d8d","_cell_guid":"92f758ee-41ca-43f3-9b12-e0219ec92891","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Building","metadata":{"_uuid":"276c65d6-be4c-4bdd-a830-7ee689e0fdd2","_cell_guid":"34662b23-9dd5-4d13-a28d-3469c73541b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"\n# Set device and optimize GPU usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True  # Speed up training when input sizes don't change\n\nprint(f\"Using device: {device}\")\n\n# --- Improved Data Loading & Preprocessing --- #\ndef read_data(path):\n    \"\"\"More efficient data loading with proper error handling\"\"\"\n    try:\n        df = pd.read_csv(path, sep='\\t', header=None)\n        valid_rows = []\n        \n        for i, row in df.iterrows():\n            if len(row) >= 2:\n                target, source = row[0], row[1]\n                if isinstance(target, str) and isinstance(source, str):\n                    valid_rows.append((target.strip(), source.strip()))\n                else:\n                    print(f\"Skipped non-string row at index {i}\")\n            else:\n                print(f\"Skipped malformed row at index {i}\")\n                \n        print(f\"Loaded {len(valid_rows)} valid rows from {path}\")\n        return valid_rows\n    except Exception as e:\n        print(f\"Error loading data from {path}: {e}\")\n        return []\n\ndef build_vocab(pairs, idx=0):\n    \"\"\"Build vocabulary dictionaries with special tokens\"\"\"\n    # Add special tokens\n    chars = sorted(set(ch for p in pairs for ch in p[idx] if isinstance(p[idx], str)))\n    stoi = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    # Add regular characters\n    for i, ch in enumerate(chars):\n        stoi[ch] = i + 4  # Start after special tokens\n    itos = {i: ch for ch, i in stoi.items()}\n    return stoi, itos\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, data, input_stoi, target_stoi):\n        self.data = data\n        self.input_stoi = input_stoi\n        self.target_stoi = target_stoi\n        \n        # Pre-process all data at init time to speed up training\n        self.processed_data = []\n        for x, y in data:\n            try:\n                # Handle unknown characters with <unk> token\n                x_indices = [self.input_stoi.get(c, self.input_stoi['<unk>']) for c in x]\n                y_indices = [self.target_stoi['<sos>']] + [self.target_stoi.get(c, self.target_stoi['<unk>']) for c in y] + [self.target_stoi['<eos>']]\n                self.processed_data.append((torch.tensor(x_indices), torch.tensor(y_indices)))\n            except Exception as e:\n                print(f\"Error processing data pair: {x}, {y}. Error: {e}\")\n                continue\n\n    def __len__(self):\n        return len(self.processed_data)\n\n    def __getitem__(self, idx):\n        return self.processed_data[idx]\n\ndef collate_fn(batch):\n    \"\"\"Efficient batch collation - keep tensors on CPU for pin_memory to work\"\"\"\n    src_batch, tgt_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    return src_batch, tgt_batch  # Return CPU tensors, don't send to device here\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, cell_type='LSTM'):\n        super(Seq2Seq, self).__init__()\n\n        # Embedding layers\n        self.encoder_embedding = nn.Embedding(input_dim, emb_dim)\n        self.decoder_embedding = nn.Embedding(output_dim, emb_dim)\n\n        # Select RNN Cell type dynamically (LSTM / GRU / RNN)\n        rnn_cell = getattr(nn, cell_type)\n\n        # Encoder and Decoder RNNs\n        self.encoder = rnn_cell(input_size=emb_dim, hidden_size=hid_dim, num_layers=n_layers, \n                            dropout=dropout if n_layers > 1 else 0, batch_first=True)\n        self.decoder = rnn_cell(input_size=emb_dim, hidden_size=hid_dim, num_layers=n_layers, \n                            dropout=dropout if n_layers > 1 else 0, batch_first=True)\n\n        # Linear layer to map decoder hidden state to output vocabulary\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n        # Store parameters for sanity checks\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        self.cell_type = cell_type\n\n        if torch.cuda.is_available():\n            torch.backends.cudnn.benchmark = True  # Enable cudnn auto-tuning\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        \"\"\"\n        src: [batch_size, src_len]\n        trg: [batch_size, trg_len]\n        \"\"\"\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        output_dim = self.output_dim\n\n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, trg_len, output_dim).to(src.device)\n\n        # Encoder embedding\n        embedded_src = self.dropout(self.encoder_embedding(src))  # [batch_size, src_len, emb_dim]\n\n        # Encoder RNN\n        _, hidden = self.encoder(embedded_src)\n\n        # First input to decoder is the <sos> token (first token in target sequence)\n        input = trg[:, 0]  # [batch_size]\n\n        for t in range(1, trg_len):\n            # Decoder embedding for current input\n            embedded = self.dropout(self.decoder_embedding(input))  # [batch_size, emb_dim]\n            embedded = embedded.unsqueeze(1)  # Add seq_len dimension: [batch_size, 1, emb_dim]\n\n            # Decoder RNN step\n            output, hidden = self.decoder(embedded, hidden)  # output: [batch_size, 1, hid_dim]\n\n            # Predict next token\n            prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n\n            # Store prediction\n            outputs[:, t, :] = prediction\n\n            # Decide if using teacher forcing for next step\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            \n            # Get top predicted token\n            top1 = prediction.argmax(1)  # [batch_size]\n\n            # Next input is either actual next token (teacher forcing) or predicted one\n            input = trg[:, t] if teacher_force else top1\n\n        return outputs\n\n    def encode(self, src):\n        \"\"\"Encode the source sequence\"\"\"\n        # Embed source sequence\n        embedded_src = self.encoder_embedding(src)  # [batch_size, src_len, emb_dim]\n        \n        # Pass through encoder\n        _, hidden = self.encoder(embedded_src)\n        \n        return hidden\n\n    def decode_step(self, input_token, hidden):\n        \"\"\"Decode one step given previous hidden state and current input token\"\"\"\n        # Embed the input token\n        embedded = self.decoder_embedding(input_token)  # [batch_size, 1, emb_dim]\n    \n        # Run through decoder\n        output, hidden = self.decoder(embedded, hidden)  # [batch_size, 1, hidden_dim]\n    \n        # Project to output vocabulary\n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, vocab_size]\n    \n        return prediction, hidden\n\n\n# --- Improved Training & Evaluation --- #\ndef train_one_epoch(model, loader, criterion, optimizer, teacher_forcing_ratio=0.7, device_to_use=None):\n    \"\"\"\n    Train the model for one epoch with a fixed teacher forcing ratio\n    \n    Args:\n        model: The Seq2Seq model\n        loader: DataLoader for training data\n        criterion: Loss function\n        optimizer: Optimizer\n        teacher_forcing_ratio: Fixed probability of using teacher forcing (default: 0.7)\n        device_to_use: Device to use for training\n        \n    Returns:\n        tuple: Average loss and accuracy for the epoch\n    \"\"\"\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n        \n    model.train()\n    total_loss, total_correct, total_chars = 0, 0, 0\n    \n    # Use tqdm for progress monitoring\n    pbar = tqdm(loader, desc=f\"Training (TF ratio: {teacher_forcing_ratio:.2f})\")\n    for src, tgt in pbar:\n        # Safety check for empty batches or invalid dimensions\n        if src.numel() == 0 or tgt.numel() == 0:\n            print(\"Warning: Empty batch detected, skipping\")\n            continue\n            \n        try:\n            # Move data to device here (after collate_fn)\n            src, tgt = src.to(device_to_use), tgt.to(device_to_use)\n            \n            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n            \n            # Use the fixed teacher forcing ratio\n            output = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n            \n            # Reshape output and target for loss calculation\n            # Output shape: [batch_size, trg_len, output_dim]\n            # We need to match this with the target format\n            # Target should exclude the first token (SOS)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].contiguous().view(-1, output_dim)\n            target = tgt[:, 1:].contiguous().view(-1)\n            \n            loss = criterion(output, target)\n            loss.backward()\n            \n            # Gradient clipping to prevent exploding gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            # Calculate accuracy\n            with torch.no_grad():\n                pred = output.argmax(dim=-1)\n                mask = target != 0  # Only count non-padding positions\n                if mask.sum().item() > 0:  # Avoid division by zero\n                    correct = (pred[mask] == target[mask]).sum().item()\n                    chars = mask.sum().item()\n                    \n                    total_correct += correct\n                    total_chars += chars\n                    total_loss += loss.item()\n                    \n                    # Update progress bar with current metrics\n                    pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct/max(1, chars):.4f}\")\n                \n        except RuntimeError as e:\n            print(f\"Error in training batch: {e}\")\n            print(f\"src shape: {src.shape}, tgt shape: {tgt.shape}\")\n            # Try to continue with next batch\n            continue\n\n    # Avoid division by zero\n    avg_loss = total_loss / max(1, len(loader))\n    avg_acc = total_correct / max(1, total_chars)\n    return avg_loss, avg_acc\ndef evaluate(model, loader, criterion, device_to_use=None):\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n        \n    model.eval()\n    total_loss, total_correct, total_chars = 0, 0, 0\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(loader, desc=\"Evaluating\"):\n            # Safety check for empty batches\n            if src.numel() == 0 or tgt.numel() == 0:\n                continue\n                \n            try:\n                # Move data to device here\n                src, tgt = src.to(device_to_use), tgt.to(device_to_use)\n                \n                output = model(src, tgt, teacher_forcing_ratio=0.0)  # No teacher forcing during evaluation\n                \n                # Reshape for loss calculation\n                output_dim = output.shape[-1]\n                output = output[:, 1:].contiguous().view(-1, output_dim)\n                target = tgt[:, 1:].contiguous().view(-1)\n                \n                loss = criterion(output, target)\n                \n                pred = output.argmax(dim=-1)\n                mask = target != 0  # Only count non-padding positions\n                if mask.sum().item() > 0:  # Avoid division by zero\n                    total_correct += (pred[mask] == target[mask]).sum().item()\n                    total_chars += mask.sum().item()\n                    total_loss += loss.item()\n                \n            except RuntimeError as e:\n                print(f\"Error in evaluation batch: {e}\")\n                continue\n\n    # Avoid division by zero\n    avg_loss = total_loss / max(1, len(loader))\n    avg_acc = total_correct / max(1, total_chars)\n    return avg_loss, avg_acc\n\n# --- Greedy Decoding --- #\ndef predict_greedy(model, input_str, input_stoi, target_stoi, target_itos, device_to_use=None):\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n        \n    model.eval()\n    input_indices = [input_stoi.get(c, input_stoi['<unk>']) for c in input_str]\n    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device_to_use)\n\n    with torch.no_grad():\n        output_indices = [target_stoi['<sos>']]\n        max_length = 100\n        \n        hidden = model.encode(input_tensor)\n        \n        for _ in range(max_length):\n            last_token = torch.tensor([[output_indices[-1]]]).to(device_to_use)\n            prediction, hidden = model.decode_step(last_token, hidden)\n            next_token = prediction.argmax(1).item()\n            \n            if next_token == target_stoi['<eos>']:\n                break\n                \n            output_indices.append(next_token)\n\n    # Convert indices to characters (skip <sos>)\n    predicted_chars = [target_itos.get(i, '<unk>') for i in output_indices[1:]]\n    predicted_str = ''.join(predicted_chars).strip()\n    \n    return predicted_str\n\n# --- Beam Search Implementation --- #\ndef predict_beam_search(model, input_str, input_stoi, target_stoi, target_itos, beam_size=5, device_to_use=None):\n    if device_to_use is None:\n        device_to_use = device\n\n    model.eval()\n    input_indices = [input_stoi.get(c, input_stoi['<unk>']) for c in input_str]\n    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device_to_use)\n\n    with torch.no_grad():\n        hidden = model.encode(input_tensor)\n\n        sos_token = target_stoi['<sos>']\n        eos_token = target_stoi['<eos>']\n\n        beams = [(0.0, [sos_token], hidden)]\n        completed_sequences = []\n        max_length = min(100, len(input_str) * 2)\n\n        for _ in range(max_length):\n            if len(completed_sequences) >= beam_size:\n                break\n\n            new_beams = []\n\n            for score, seq, hidden_state in beams:\n                if seq[-1] == eos_token:\n                    completed_sequences.append((score, seq))\n                    continue\n\n                last_token = torch.tensor([[seq[-1]]]).to(device_to_use)\n                prediction, new_hidden = model.decode_step(last_token, hidden_state)\n\n                log_probs = torch.log_softmax(prediction.squeeze(0), dim=-1)\n                topk_probs, topk_idxs = torch.topk(log_probs, min(beam_size, log_probs.size(-1)))\n\n                for i in range(topk_idxs.size(-1)):\n                    prob = topk_probs[i].item()\n                    idx = topk_idxs[i].item()\n\n                    new_score = score + prob\n                    new_seq = seq + [idx]\n\n                    if idx == eos_token:\n                        completed_sequences.append((new_score, new_seq))\n                    else:\n                        new_beams.append((new_score, new_seq, new_hidden))\n\n            beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[0])\n            if not beams:\n                break\n\n        if not completed_sequences:\n            if beams:\n                best_beam = max(beams, key=lambda x: x[0])\n                completed_sequences.append((best_beam[0], best_beam[1]))\n            else:\n                completed_sequences.append((0.0, [sos_token]))\n\n        best_score, best_seq = max(completed_sequences, key=lambda x: x[0])\n\n        if best_seq[-1] == eos_token:\n            best_seq = best_seq[1:-1]\n        else:\n            best_seq = best_seq[1:]\n\n    # Convert indices back to characters\n    predicted_chars = [target_itos.get(i, '<unk>') for i in best_seq]\n    predicted_str = ''.join(predicted_chars).strip()   # <-- ✅ this is the critical addition\n\n    return predicted_str\n\n# --- WandB Sweep Training Loop with Meaningful Run Names --- #\ndef sweep_train(config=None):\n    # Initialize wandb with exception handling\n    try:\n        with wandb.init(config=config) as run:\n            config = wandb.config\n            \n            # Create meaningful name for the run based on config\n            # Include the fixed teacher forcing ratio in the run name\n            teacher_forcing_ratio = 0.7  # Fixed teacher forcing ratio\n            run_name = f\"{config.cell_type}-L{config.n_layers}-H{config.hid_dim}-E{config.emb_dim}-D{config.dropout}-B{config.beam_size}-TF{teacher_forcing_ratio}\"\n            run.name = run_name  # Set the name for WandB run\n            \n            print(f\"Training with config: {config}, TF ratio: {teacher_forcing_ratio}, Run name: {run_name}\")\n            \n            # Create model with better error handling\n            try:\n                # Make sure input_stoi and target_stoi are defined\n                if 'input_stoi' not in globals() or 'target_stoi' not in globals():\n                    print(\"Warning: Vocabularies not defined. Make sure to run the data loading code first.\")\n                    return\n                \n                model = Seq2Seq(\n                    input_dim=len(input_stoi),\n                    output_dim=len(target_stoi),\n                    emb_dim=config.emb_dim,\n                    hid_dim=config.hid_dim,\n                    n_layers=config.n_layers,\n                    cell_type=config.cell_type,\n                    dropout=config.dropout\n                )\n                \n                # Print model details before moving to GPU\n                total_params = sum(p.numel() for p in model.parameters())\n                print(f\"Model has {total_params:,} parameters\")\n                \n                # IMPORTANT FIX: Always start on CPU first, then try GPU\n                # This prevents CUDA errors during model initialization\n                model = model.to('cpu')\n                device_to_use = torch.device('cpu')\n                \n                # Try to move to GPU if available\n                if torch.cuda.is_available():\n                    try:\n                        # Set environment variable for better error messages\n                        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n                        model = model.to(device)\n                        device_to_use = device\n                        print(\"Successfully moved model to GPU\")\n                    except RuntimeError as e:\n                        print(f\"CUDA error when moving model to device: {e}\")\n                        print(\"Keeping model on CPU\")\n                \n                # Initialize optimizer and loss function\n                optimizer = optim.Adam(model.parameters(), lr=0.001)\n                # KEY FIX: Use ignore_index=0 to ignore padding in loss computation\n                criterion = nn.CrossEntropyLoss(ignore_index=0)\n                \n                best_val_acc = 0\n                best_config = None\n                for epoch in range(3):  # 3 epochs\n                    print(f\"\\nEpoch {epoch+1}/3\")\n                    \n                    try:\n                        # Use device_to_use instead of global device and pass the fixed teacher forcing ratio\n                        train_loss, train_acc = train_one_epoch(\n                            model, \n                            train_loader, \n                            criterion, \n                            optimizer, \n                            teacher_forcing_ratio=teacher_forcing_ratio,  # Pass fixed value\n                            device_to_use=device_to_use\n                        )\n                        val_loss, val_acc = evaluate(model, dev_loader, criterion, device_to_use)\n                        \n                        # Log the teacher forcing ratio along with other metrics\n                        wandb.log({\n                            \"train_loss\": train_loss, \n                            \"train_acc\": train_acc,\n                            \"val_loss\": val_loss, \n                            \"val_acc\": val_acc,\n                            \"teacher_forcing_ratio\": teacher_forcing_ratio  # Log the TF ratio\n                        })\n                        \n                        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n                        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n                        \n                        # Save best model\n                        if val_acc > best_val_acc:\n                            best_val_acc = val_acc\n                            best_config = dict(config)  # Clone the best config\n                            best_config['teacher_forcing_ratio'] = teacher_forcing_ratio  # Add TF ratio to the best config\n\n                    except Exception as e:\n                        print(f\"Error during epoch {epoch+1}: {e}\")\n                        import traceback\n                        traceback.print_exc()\n                        continue\n                \n                # At end of training\n                if best_config:\n                    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n                    print(\"Best config:\")\n                    for k, v in best_config.items():\n                        print(f\"{k}: {v}\")\n\n                # Evaluation with beam search (if there was a successful training)\n                if best_val_acc > 0 and hasattr(config, 'beam_size') and config.beam_size > 1:\n                    print(f\"\\nEvaluating with beam search (beam size = {config.beam_size})...\")\n                    \n                    # Use a small subset for beam search evaluation to save time\n                    num_samples = min(50, len(test_data))\n                    beam_search_examples = test_data[:num_samples]\n                    \n                    # Compare greedy vs beam decoding\n                    greedy_correct = 0\n                    beam_correct = 0\n                    \n                    for target, source in tqdm(beam_search_examples):\n                        try:\n                            greedy_output = predict_greedy(model, source, input_stoi, target_stoi, target_itos, device_to_use)\n                            beam_output = predict_beam_search(model, source, input_stoi, target_stoi, target_itos, beam_size=config.beam_size, device_to_use=device_to_use)\n                            \n                            greedy_correct += 1 if greedy_output.strip() == target.strip() else 0\n                            beam_correct   += 1 if beam_output.strip() == target.strip() else 0\n                        except Exception as e:\n                            print(f\"Error during prediction: {e}\")\n                            continue\n                    \n                    greedy_acc = greedy_correct / num_samples\n                    beam_acc = beam_correct / num_samples\n                    \n                    wandb.log({\n                        \"greedy_accuracy\": greedy_acc,\n                        \"beam_search_accuracy\": beam_acc\n                    })\n                    \n                    print(f\"Greedy Decoding Accuracy: {greedy_acc:.4f}\")\n                    print(f\"Beam Search Accuracy: {beam_acc:.4f}\")\n            \n            except Exception as e:\n                print(f\"Fatal error during model training: {e}\")\n                import traceback\n                traceback.print_exc()\n                \n    except Exception as e:\n        print(f\"Error initializing wandb: {e}\")\n\n\n# --- Main Execution --- #\n\nif __name__ == \"__main__\":\n    # Add debugging for CUDA device initialization\n    if torch.cuda.is_available():\n        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n        print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n\n    # --- Load Data --- #\n    print(\"Loading data...\")\n    train_data = read_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv')\n    dev_data = read_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv')\n    test_data = read_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv')\n\n    print(f\"Data loaded - Train: {len(train_data)}, Dev: {len(dev_data)}, Test: {len(test_data)}\")\n\n    # Build vocab with special tokens\n    print(\"Building vocabulary...\")\n    input_stoi, input_itos = build_vocab(train_data, idx=1)  # Roman input\n    target_stoi, target_itos = build_vocab(train_data, idx=0)  # Telugu output\n\n    print(f\"Vocabulary size - Input: {len(input_stoi)}, Target: {len(target_stoi)}\")\n\n    # --- Dataset and DataLoader preparation --- #\n    print(\"Preparing datasets...\")\n    train_ds = TransliterationDataset(train_data, input_stoi, target_stoi)\n    dev_ds = TransliterationDataset(dev_data, input_stoi, target_stoi)\n    test_ds = TransliterationDataset(test_data, input_stoi, target_stoi)\n\n    # Check if we have valid data\n    if len(train_ds) == 0:\n        print(\"Error: No valid training data after processing\")\n        raise ValueError(\"Empty dataset\")\n\n    # Optimize batch size for better GPU utilization\n    # KEY FIX: Reduced batch size to avoid memory issues\n    BATCH_SIZE = 32  # Further reduced from 64 to be safer\n    # Use pin_memory for faster data loading, but only if using CUDA\n    pin_memory = torch.cuda.is_available()\n\n    train_loader = DataLoader(\n        train_ds, \n        batch_size=BATCH_SIZE, \n        shuffle=True, \n        collate_fn=collate_fn,\n        pin_memory=pin_memory\n    )\n\n    dev_loader = DataLoader(\n        dev_ds, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        collate_fn=collate_fn,\n        pin_memory=pin_memory\n    )\n\n    test_loader = DataLoader(\n        test_ds, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        collate_fn=collate_fn,\n        pin_memory=pin_memory\n    )\n\n    # --- Sweep Configuration with Beam Size --- #\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {\n            'name': 'val_acc',\n            'goal': 'maximize'\n        },\n        'parameters': {\n            'emb_dim': {'values': [32, 64, 128]},\n            'hid_dim': {'values': [64, 128, 256]},\n            'n_layers': {'values': [1, 2]},\n            'dropout': {'values': [0.2, 0.3]},\n            'cell_type': {'values': ['LSTM', 'GRU','RNN']},\n            'beam_size': {'values': [1, 3]}\n        }\n    }\n\n    # --- Run WandB Sweep --- #\n    print(\"Starting WandB sweep...\")\n    # Reduced count to 5 for initial testing\n\n    sweep_id = wandb.sweep(sweep_config, project=\"DA6401-A3\")\n    wandb.agent(sweep_id, function=sweep_train, count=25)  # Start with just 5 runs","metadata":{"_uuid":"043682ac-a596-42d2-81da-84e0dc95a124","_cell_guid":"d8dc3a38-4385-4321-95d8-56fdf7c725e5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\n# Load best config manually after sweep (fill your values)\nbest_config = {\n    'emb_dim': 64,\n    'hid_dim': 256,\n    'n_layers': 2,\n    'dropout': 0.3,\n    'cell_type': 'LSTM'\n}\n\n# Combine train and dev data for final training\nfinal_train_data = train_data + dev_data\nfinal_train_ds = TransliterationDataset(final_train_data, input_stoi, target_stoi)\nfinal_train_loader = DataLoader(\n    final_train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn, pin_memory=pin_memory\n)\n\n# Initialize model\nfinal_model = Seq2Seq(\n    input_dim=len(input_stoi),\n    output_dim=len(target_stoi),\n    emb_dim=best_config['emb_dim'],\n    hid_dim=best_config['hid_dim'],\n    n_layers=best_config['n_layers'],\n    dropout=best_config['dropout'],\n    cell_type=best_config['cell_type']\n).to(device)\n\n# Optimizer and loss\noptimizer = torch.optim.Adam(final_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\n# Initialize wandb run\nwandb.init(project=\"DA6401-A3\", name=\"Test-Run\", config=best_config, allow_val_change=True, reinit=True)\n\n\n# Final training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_one_epoch(final_model, final_train_loader, criterion, optimizer)\n    print(f\"Epoch {epoch+1}/{EPOCHS} — Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n\n    wandb.log({\n        'epoch': epoch+1,\n        'train_loss': train_loss,\n        'train_acc': train_acc\n    })","metadata":{"_uuid":"14687d22-bd76-4afe-987f-9195e7435914","_cell_guid":"c98ccbe1-367b-424e-8a4c-063e8d5b6dbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport wandb\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Create predictions folder\nos.makedirs(\"predictions_vanilla\", exist_ok=True)\npred_file_path = os.path.join(\"predictions_vanilla\", \"test_predictions.txt\")\n\n# Start a new WandB run to log predictions\nwandb.init(project=\"DA6401-A3\", name=\"Final-Test-Predictions\")\n\n# Final evaluation\nfinal_model.eval()\ntotal, correct = 0, 0\nprediction_records = []\nprint(f\"Number of test samples: {len(test_data)}\")\nwith open(pred_file_path, 'w', encoding='utf-8') as f:\n    for target, source in test_data:\n        beam_size = best_config.get('beam_size', 1)\n        if beam_size > 1:\n            pred = predict_beam_search(final_model, source, input_stoi, target_stoi, target_itos, beam_size=beam_size)\n        else:\n            pred = predict_greedy(final_model, source, input_stoi, target_stoi, target_itos)\n\n        # print(f\"Source: {source} | Predicted: {pred} | Target: {target}\")\n        # break\n\n\n        # Record for both WandB and file\n        record = {\n            \"Input (Romanized)\": source,\n            \"Model Prediction (Telugu)\": pred,\n            \"Reference (Telugu)\": target\n        }\n        prediction_records.append(record)\n        if total < 20:\n            print(f\"Source: {source} | Predicted: {pred} | Target: {target}\")\n\n\n        # Write to file\n        f.write(f\"{source}\\t{pred}\\n\")\n\n        # Accuracy calculation\n        if pred.strip() == target.strip():\n            correct += 1\n        total += 1\n\n# Compute final test accuracy\ntest_acc = correct / total\nprint(f\"\\nFinal Test Accuracy (Exact Match): {test_acc:.4f}\")\n\n# Log final test accuracy\nwandb.log({\"final_test_accuracy\": test_acc})\n\n# Log creative predictions table\npred_df = pd.DataFrame(prediction_records)\nwandb_table = wandb.Table(dataframe=pred_df)\nwandb.log({\"final_test_predictions_table\": wandb_table})\n\nwandb.finish()","metadata":{"_uuid":"4ea3d8c4-6a34-4cd6-82cb-10b37614db11","_cell_guid":"e9ffd7ca-95ff-4aab-9f1e-5ffaeecdea6e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attension","metadata":{}},{"cell_type":"code","source":"\n# Set device and optimize GPU usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True  # Speed up training when input sizes don't change\n\nprint(f\"Using device: {device}\")\n\n# --- Improved Data Loading & Preprocessing --- #\ndef read_data(path):\n    \"\"\"More efficient data loading with proper error handling\"\"\"\n    try:\n        df = pd.read_csv(path, sep='\\t', header=None)\n        valid_rows = []\n        \n        for i, row in df.iterrows():\n            if len(row) >= 2:\n                target, source = row[0], row[1]\n                if isinstance(target, str) and isinstance(source, str):\n                    valid_rows.append((target.strip(), source.strip()))\n                else:\n                    print(f\"Skipped non-string row at index {i}\")\n            else:\n                print(f\"Skipped malformed row at index {i}\")\n                \n        print(f\"Loaded {len(valid_rows)} valid rows from {path}\")\n        return valid_rows\n    except Exception as e:\n        print(f\"Error loading data from {path}: {e}\")\n        return []\n\ndef build_vocab(pairs, idx=0):\n    \"\"\"Build vocabulary dictionaries with special tokens\"\"\"\n    # Add special tokens\n    chars = sorted(set(ch for p in pairs for ch in p[idx] if isinstance(p[idx], str)))\n    stoi = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    # Add regular characters\n    for i, ch in enumerate(chars):\n        stoi[ch] = i + 4  # Start after special tokens\n    itos = {i: ch for ch, i in stoi.items()}\n    return stoi, itos\n\nclass TransliterationDataset(Dataset):\n    def __init__(self, data, input_stoi, target_stoi):\n        self.data = data\n        self.input_stoi = input_stoi\n        self.target_stoi = target_stoi\n        \n        # Pre-process all data at init time to speed up training\n        self.processed_data = []\n        for x, y in data:\n            try:\n                # Handle unknown characters with <unk> token\n                x_indices = [self.input_stoi.get(c, self.input_stoi['<unk>']) for c in x]\n                y_indices = [self.target_stoi['<sos>']] + [self.target_stoi.get(c, self.target_stoi['<unk>']) for c in y] + [self.target_stoi['<eos>']]\n                self.processed_data.append((torch.tensor(x_indices), torch.tensor(y_indices)))\n            except Exception as e:\n                print(f\"Error processing data pair: {x}, {y}. Error: {e}\")\n                continue\n\n    def __len__(self):\n        return len(self.processed_data)\n\n    def __getitem__(self, idx):\n        return self.processed_data[idx]\n\ndef collate_fn(batch):\n    \"\"\"Efficient batch collation - keep tensors on CPU for pin_memory to work\"\"\"\n    src_batch, tgt_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    return src_batch, tgt_batch  # Return CPU tensors, don't send to device here\n# First, let's import the missing F module that we use in the attention class\nimport torch.nn.functional as F\n\n# Fixed implementation of the Attention class\nclass Attention(nn.Module):\n    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n        super().__init__()\n        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n\n    def forward(self, hidden, encoder_outputs):\n        # Check dimensions of hidden and handle appropriately\n        batch_size = encoder_outputs.shape[0]\n        src_len = encoder_outputs.shape[1]\n        \n        # Fix: Check if hidden has 3 dimensions [n_layers, batch, hidden_dim]\n        if hidden.dim() == 3:\n            hidden = hidden[-1]  # Use only the last layer: [batch_size, dec_hidden_dim]\n            \n        # Now reshape hidden for attention calculation\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch, src_len, dec_hidden_dim]\n\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch, src_len, dec_hidden_dim] \n        energy = energy.permute(0, 2, 1)  # [batch, dec_hidden_dim, src_len]\n        \n        # Fix: Make sure v has the correct batch dimension\n        v = self.v.unsqueeze(0).repeat(batch_size, 1).unsqueeze(1)  # [batch, 1, dec_hidden_dim]\n\n        attention = torch.bmm(v, energy).squeeze(1)  # [batch, src_len]\n        return F.softmax(attention, dim=1)\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))  # [batch, src_len, emb_dim]\n        outputs, hidden = self.rnn(embedded)           # outputs: [batch, src_len, hid_dim]\n        return outputs, hidden\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.GRU(enc_hid_dim + emb_dim, dec_hid_dim, n_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(enc_hid_dim + dec_hid_dim + emb_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, encoder_outputs):\n        input = input.unsqueeze(1)  # [batch, 1]\n        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n\n        # Attention\n        attn_weights = self.attention(hidden[-1], encoder_outputs)  # [batch, src_len]\n        attn_weights = attn_weights.unsqueeze(1)  # [batch, 1, src_len]\n        context = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, enc_hid_dim]\n\n        # RNN input\n        rnn_input = torch.cat((embedded, context), dim=2)  # [batch, 1, emb_dim + enc_hid_dim]\n        output, hidden = self.rnn(rnn_input, hidden)  # output: [batch, 1, dec_hid_dim]\n\n        # Output prediction\n        output = output.squeeze(1)\n        context = context.squeeze(1)\n        embedded = embedded.squeeze(1)\n\n        pred = self.fc_out(torch.cat((output, context, embedded), dim=1))  # [batch, output_dim]\n        return pred, hidden\n\nclass Seq2Seq_atten(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        batch_size = src.size(0)\n        tgt_len = tgt.size(1)\n        tgt_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n\n        encoder_outputs, hidden = self.encoder(src)\n        input_token = tgt[:, 0]  # <sos>\n\n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input_token, hidden, encoder_outputs)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input_token = tgt[:, t] if teacher_force else top1\n\n        return outputs\n\n    def encode(self, src):\n        \"\"\"Get encoder outputs and hidden state\"\"\"\n        encoder_outputs, hidden = self.encoder(src)\n        return encoder_outputs, hidden  # Return both outputs and hidden state\n\n    def decode_step(self, input_token, encoder_state):\n        \"\"\"Perform one decoding step\"\"\"\n        # Unpack encoder state into outputs and hidden\n        encoder_outputs, hidden = encoder_state\n        \n        # Perform decoder step\n        output, hidden = self.decoder(input_token.squeeze(1), hidden, encoder_outputs)\n        \n        # Return output and new state (which includes both outputs and hidden)\n        return output, (encoder_outputs, hidden)\n\n\n\n# --- Improved Training & Evaluation --- #\ndef train_one_epoch(model, loader, criterion, optimizer, teacher_forcing_ratio=0.7, device_to_use=None):\n    \"\"\"\n    Train the model for one epoch with a fixed teacher forcing ratio\n    \n    Args:\n        model: The Seq2Seq model\n        loader: DataLoader for training data\n        criterion: Loss function\n        optimizer: Optimizer\n        teacher_forcing_ratio: Fixed probability of using teacher forcing (default: 0.7)\n        device_to_use: Device to use for training\n        \n    Returns:\n        tuple: Average loss and accuracy for the epoch\n    \"\"\"\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n        \n    # Ensure the model is on the correct device\n    model = model.to(device_to_use)\n    model.train()\n    total_loss, total_correct, total_chars = 0, 0, 0\n    \n    # Use tqdm for progress monitoring\n    pbar = tqdm(loader, desc=f\"Training (TF ratio: {teacher_forcing_ratio:.2f})\")\n    for src, tgt in pbar:\n        # Safety check for empty batches or invalid dimensions\n        if src.numel() == 0 or tgt.numel() == 0:\n            print(\"Warning: Empty batch detected, skipping\")\n            continue\n            \n        try:\n            # Make sure to move data to the correct device\n            src = src.to(device_to_use)\n            tgt = tgt.to(device_to_use)\n            \n            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n            \n            # Use the fixed teacher forcing ratio\n            output = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n            \n            # Reshape output and target for loss calculation\n            # Output shape: [batch_size, trg_len, output_dim]\n            # We need to match this with the target format\n            # Target should exclude the first token (SOS)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].contiguous().view(-1, output_dim)\n            target = tgt[:, 1:].contiguous().view(-1)\n            \n            # Double check that both are on the same device\n            if output.device != target.device:\n                print(f\"Device mismatch: output on {output.device}, target on {target.device}\")\n                target = target.to(output.device)\n            \n            loss = criterion(output, target)\n            loss.backward()\n            \n            # Gradient clipping to prevent exploding gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            # Calculate accuracy\n            with torch.no_grad():\n                pred = output.argmax(dim=-1)\n                mask = target != 0  # Only count non-padding positions\n                if mask.sum().item() > 0:  # Avoid division by zero\n                    correct = (pred[mask] == target[mask]).sum().item()\n                    chars = mask.sum().item()\n                    \n                    total_correct += correct\n                    total_chars += chars\n                    total_loss += loss.item()\n                    \n                    # Update progress bar with current metrics\n                    pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct/max(1, chars):.4f}\")\n                \n        except RuntimeError as e:\n            print(f\"Error in training batch: {e}\")\n            print(f\"src shape: {src.shape}, tgt shape: {tgt.shape}\")\n            print(f\"src device: {src.device}, tgt device: {tgt.device}\")\n            \n            # Try to recover by moving tensors to the correct device\n            try:\n                if isinstance(src, torch.Tensor) and isinstance(tgt, torch.Tensor):\n                    src = src.to(device_to_use)\n                    tgt = tgt.to(device_to_use)\n                    print(f\"Moved tensors to {device_to_use}. Continuing...\")\n                else:\n                    print(\"Cannot move tensors, skipping batch\")\n            except:\n                print(\"Recovery failed, skipping batch\")\n                \n            # Try to continue with next batch\n            continue\n\n    # Avoid division by zero\n    avg_loss = total_loss / max(1, len(loader))\n    avg_acc = total_correct / max(1, total_chars)\n    return avg_loss, avg_acc\n\ndef evaluate(model, loader, criterion, device_to_use=None):\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n    \n    # Ensure the model is on the correct device\n    model = model.to(device_to_use)    \n    model.eval()\n    total_loss, total_correct, total_chars = 0, 0, 0\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(loader, desc=\"Evaluating\"):\n            # Safety check for empty batches\n            if src.numel() == 0 or tgt.numel() == 0:\n                continue\n                \n            try:\n                # Move data to device here\n                src = src.to(device_to_use)\n                tgt = tgt.to(device_to_use)\n                \n                output = model(src, tgt, teacher_forcing_ratio=0.0)  # No teacher forcing during evaluation\n                \n                # Reshape for loss calculation\n                output_dim = output.shape[-1]\n                output = output[:, 1:].contiguous().view(-1, output_dim)\n                target = tgt[:, 1:].contiguous().view(-1)\n                \n                # Double check device match\n                if output.device != target.device:\n                    target = target.to(output.device)\n                \n                loss = criterion(output, target)\n                \n                pred = output.argmax(dim=-1)\n                mask = target != 0  # Only count non-padding positions\n                if mask.sum().item() > 0:  # Avoid division by zero\n                    total_correct += (pred[mask] == target[mask]).sum().item()\n                    total_chars += mask.sum().item()\n                    total_loss += loss.item()\n                \n            except RuntimeError as e:\n                print(f\"Error in evaluation batch: {e}\")\n                print(f\"src device: {src.device}, tgt device: {tgt.device}\")\n                continue\n\n    # Avoid division by zero\n    avg_loss = total_loss / max(1, len(loader))\n    avg_acc = total_correct / max(1, total_chars)\n    return avg_loss, avg_acc\n\ndef predict_greedy(model, input_str, input_stoi, target_stoi, target_itos, device_to_use=None):\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n    \n    # Ensure the model is on the correct device\n    model = model.to(device_to_use)    \n    model.eval()\n    input_indices = [input_stoi.get(c, input_stoi['<unk>']) for c in input_str]\n    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device_to_use)\n\n    with torch.no_grad():\n        output_indices = [target_stoi['<sos>']]\n        max_length = 100\n        \n        # Encode the input\n        encoder_state = model.encode(input_tensor)\n        \n        for _ in range(max_length):\n            last_token = torch.tensor([[output_indices[-1]]]).to(device_to_use)\n            prediction, encoder_state = model.decode_step(last_token, encoder_state)\n            next_token = prediction.argmax(1).item()\n            \n            if next_token == target_stoi['<eos>']:\n                break\n                \n            output_indices.append(next_token)\n\n    # Convert indices to characters (skip <sos>)\n    predicted_chars = [target_itos.get(i, '<unk>') for i in output_indices[1:]]\n    predicted_str = ''.join(predicted_chars).strip()\n    \n    return predicted_str\n\ndef predict_beam_search(model, input_str, input_stoi, target_stoi, target_itos, beam_size=5, device_to_use=None):\n    if device_to_use is None:\n        device_to_use = device  # Fall back to global device\n\n    # Ensure the model is on the correct device\n    model = model.to(device_to_use)\n    model.eval()\n    input_indices = [input_stoi.get(c, input_stoi['<unk>']) for c in input_str]\n    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device_to_use)\n\n    with torch.no_grad():\n        # Encode the input sequence once\n        encoder_state = model.encode(input_tensor)\n\n        sos_token = target_stoi['<sos>']\n        eos_token = target_stoi['<eos>']\n\n        # Initialize beams with the encoder state\n        beams = [(0.0, [sos_token], encoder_state)]\n        completed_sequences = []\n        max_length = min(100, len(input_str) * 2)\n\n        for _ in range(max_length):\n            if len(completed_sequences) >= beam_size:\n                break\n\n            new_beams = []\n\n            for score, seq, current_state in beams:\n                if seq[-1] == eos_token:\n                    completed_sequences.append((score, seq))\n                    continue\n\n                last_token = torch.tensor([[seq[-1]]]).to(device_to_use)\n                prediction, new_state = model.decode_step(last_token, current_state)\n\n                log_probs = torch.log_softmax(prediction.squeeze(0), dim=-1)\n                topk_probs, topk_idxs = torch.topk(log_probs, min(beam_size, log_probs.size(-1)))\n\n                for i in range(topk_idxs.size(-1)):\n                    prob = topk_probs[i].item()\n                    idx = topk_idxs[i].item()\n\n                    new_score = score + prob\n                    new_seq = seq + [idx]\n\n                    if idx == eos_token:\n                        completed_sequences.append((new_score, new_seq))\n                    else:\n                        new_beams.append((new_score, new_seq, new_state))\n\n            beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[0])\n            if not beams:\n                break\n\n        if not completed_sequences:\n            if beams:\n                best_beam = max(beams, key=lambda x: x[0])\n                completed_sequences.append((best_beam[0], best_beam[1]))\n            else:\n                completed_sequences.append((0.0, [sos_token]))\n\n        best_score, best_seq = max(completed_sequences, key=lambda x: x[0])\n\n        if best_seq[-1] == eos_token:\n            best_seq = best_seq[1:-1]\n        else:\n            best_seq = best_seq[1:]\n\n    # Convert indices back to characters\n    predicted_chars = [target_itos.get(i, '<unk>') for i in best_seq]\n    predicted_str = ''.join(predicted_chars).strip() \n\n    return predicted_str\n# --- WandB Sweep Training Loop with Meaningful Run Names --- #\ndef sweep_train(config=None):\n    # Initialize wandb with exception handling\n    try:\n        with wandb.init(config=config) as run:\n            config = wandb.config\n            \n            # Create meaningful name for the run based on config\n            # Include the fixed teacher forcing ratio in the run name\n            teacher_forcing_ratio = 0.7  # Fixed teacher forcing ratio\n            run_name = f\"{config.cell_type}-L{config.n_layers}-H{config.hid_dim}-E{config.emb_dim}-D{config.dropout}-B{config.beam_size}-TF{teacher_forcing_ratio}\"\n            run.name = run_name  # Set the name for WandB run\n            \n            print(f\"Training with config: {config}, TF ratio: {teacher_forcing_ratio}, Run name: {run_name}\")\n            \n            # Determine device to use first, so we can create models on it directly\n            device_to_use = torch.device('cpu')\n            if torch.cuda.is_available():\n                try:\n                    device_to_use = torch.device('cuda')\n                    # Set environment variable for better error messages\n                    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n                    print(f\"Using device: {device_to_use}\")\n                except RuntimeError as e:\n                    print(f\"CUDA error when setting device: {e}\")\n                    print(\"Falling back to CPU\")\n                    device_to_use = torch.device('cpu')\n            \n            # Create model with better error handling\n            try:\n                # Make sure input_stoi and target_stoi are defined\n                if 'input_stoi' not in globals() or 'target_stoi' not in globals():\n                    print(\"Warning: Vocabularies not defined. Make sure to run the data loading code first.\")\n                    return\n                \n                # First create encoder and decoder separately, already on the target device\n                encoder = Encoder(\n                    input_dim=len(input_stoi),\n                    emb_dim=config.emb_dim,\n                    hid_dim=config.hid_dim,\n                    n_layers=config.n_layers,\n                    dropout=config.dropout\n                ).to(device_to_use)\n                \n                # Create attention mechanism\n                attention = Attention(\n                    enc_hidden_dim=config.hid_dim,\n                    dec_hidden_dim=config.hid_dim\n                ).to(device_to_use)\n                \n                # Create decoder\n                decoder = Decoder(\n                    output_dim=len(target_stoi),\n                    emb_dim=config.emb_dim,\n                    enc_hid_dim=config.hid_dim,\n                    dec_hid_dim=config.hid_dim,\n                    n_layers=config.n_layers,\n                    dropout=config.dropout,\n                    attention=attention\n                ).to(device_to_use)\n                \n                # Now create the Seq2Seq model with encoder and decoder\n                model = Seq2Seq_atten(\n                    encoder=encoder,\n                    decoder=decoder,\n                    device=device_to_use  # Use the device we determined earlier\n                ).to(device_to_use)  # Explicitly move the model to the device\n                \n                # Print model details\n                total_params = sum(p.numel() for p in model.parameters())\n                print(f\"Model has {total_params:,} parameters\")\n                print(f\"Model is on device: {next(model.parameters()).device}\")\n                \n                # Initialize optimizer and loss function\n                optimizer = optim.Adam(model.parameters(), lr=0.001)\n                criterion = nn.CrossEntropyLoss(ignore_index=0)\n                criterion = criterion.to(device_to_use)  # Move criterion to device as well\n                \n                best_val_acc = 0\n                best_config = None\n                for epoch in range(3):  # 3 epochs\n                    print(f\"\\nEpoch {epoch+1}/3\")\n                    \n                    try:\n                        # Use device_to_use for all operations\n                        train_loss, train_acc = train_one_epoch(\n                            model, \n                            train_loader, \n                            criterion, \n                            optimizer, \n                            teacher_forcing_ratio=teacher_forcing_ratio,\n                            device_to_use=device_to_use\n                        )\n                        val_loss, val_acc = evaluate(model, dev_loader, criterion, device_to_use)\n                        \n                        # Log the teacher forcing ratio along with other metrics\n                        wandb.log({\n                            \"train_loss\": train_loss, \n                            \"train_acc\": train_acc,\n                            \"val_loss\": val_loss, \n                            \"val_acc\": val_acc,\n                            \"teacher_forcing_ratio\": teacher_forcing_ratio\n                        })\n                        \n                        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n                        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n                        \n                        # Save best model\n                        if val_acc > best_val_acc:\n                            best_val_acc = val_acc\n                            best_config = dict(config)  # Clone the best config\n                            best_config['teacher_forcing_ratio'] = teacher_forcing_ratio  # Add TF ratio to the best config\n\n                    except Exception as e:\n                        print(f\"Error during epoch {epoch+1}: {e}\")\n                        import traceback\n                        traceback.print_exc()\n                        continue\n                \n                # At end of training\n                if best_config:\n                    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n                    print(\"Best config:\")\n                    for k, v in best_config.items():\n                        print(f\"{k}: {v}\")\n\n                # Evaluation with beam search (if there was a successful training)\n                if best_val_acc > 0 and hasattr(config, 'beam_size') and config.beam_size > 1:\n                    print(f\"\\nEvaluating with beam search (beam size = {config.beam_size})...\")\n                    \n                    # Use a small subset for beam search evaluation to save time\n                    num_samples = min(50, len(test_data))\n                    beam_search_examples = test_data[:num_samples]\n                    \n                    # Compare greedy vs beam decoding\n                    greedy_correct = 0\n                    beam_correct = 0\n                    \n                    for target, source in tqdm(beam_search_examples):\n                        try:\n                            greedy_output = predict_greedy(model, source, input_stoi, target_stoi, target_itos, device_to_use)\n                            beam_output = predict_beam_search(model, source, input_stoi, target_stoi, target_itos, beam_size=config.beam_size, device_to_use=device_to_use)\n                            \n                            greedy_correct += 1 if greedy_output.strip() == target.strip() else 0\n                            beam_correct   += 1 if beam_output.strip() == target.strip() else 0\n                        except Exception as e:\n                            print(f\"Error during prediction: {e}\")\n                            continue\n                    \n                    greedy_acc = greedy_correct / num_samples\n                    beam_acc = beam_correct / num_samples\n                    \n                    wandb.log({\n                        \"greedy_accuracy\": greedy_acc,\n                        \"beam_search_accuracy\": beam_acc\n                    })\n                    \n                    print(f\"Greedy Decoding Accuracy: {greedy_acc:.4f}\")\n                    print(f\"Beam Search Accuracy: {beam_acc:.4f}\")\n            \n            except Exception as e:\n                print(f\"Fatal error during model training: {e}\")\n                import traceback\n                traceback.print_exc()\n                \n    except Exception as e:\n        print(f\"Error initializing wandb: {e}\")\n# --- Main Execution --- #\n\nif __name__ == \"__main__\":\n    # Add debugging for CUDA device initialization\n    if torch.cuda.is_available():\n        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n        print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n\n    # --- Load Data --- #\n    print(\"Loading data...\")\n    train_data = read_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv')\n    dev_data = read_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv')\n    test_data = read_data('/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv')\n\n    print(f\"Data loaded - Train: {len(train_data)}, Dev: {len(dev_data)}, Test: {len(test_data)}\")\n\n    # Build vocab with special tokens\n    print(\"Building vocabulary...\")\n    input_stoi, input_itos = build_vocab(train_data, idx=1)  # Roman input\n    target_stoi, target_itos = build_vocab(train_data, idx=0)  # Telugu output\n\n    print(f\"Vocabulary size - Input: {len(input_stoi)}, Target: {len(target_stoi)}\")\n\n    # --- Dataset and DataLoader preparation --- #\n    print(\"Preparing datasets...\")\n    train_ds = TransliterationDataset(train_data, input_stoi, target_stoi)\n    dev_ds = TransliterationDataset(dev_data, input_stoi, target_stoi)\n    test_ds = TransliterationDataset(test_data, input_stoi, target_stoi)\n\n    # Check if we have valid data\n    if len(train_ds) == 0:\n        print(\"Error: No valid training data after processing\")\n        raise ValueError(\"Empty dataset\")\n\n    # Optimize batch size for better GPU utilization\n    # KEY FIX: Reduced batch size to avoid memory issues\n    BATCH_SIZE = 32  # Further reduced from 64 to be safer\n    # Use pin_memory for faster data loading, but only if using CUDA\n    pin_memory = torch.cuda.is_available()\n\n    train_loader = DataLoader(\n        train_ds, \n        batch_size=BATCH_SIZE, \n        shuffle=True, \n        collate_fn=collate_fn,\n        pin_memory=pin_memory\n    )\n\n    dev_loader = DataLoader(\n        dev_ds, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        collate_fn=collate_fn,\n        pin_memory=pin_memory\n    )\n\n    test_loader = DataLoader(\n        test_ds, \n        batch_size=BATCH_SIZE, \n        shuffle=False, \n        collate_fn=collate_fn,\n        pin_memory=pin_memory\n    )\n\n    # --- Sweep Configuration with Beam Size --- #\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {\n            'name': 'val_acc',\n            'goal': 'maximize'\n        },\n        'parameters': {\n            'emb_dim': {'values': [32, 64, 128]},\n            'hid_dim': {'values': [64, 128, 256]},\n            'n_layers': {'values': [1, 2]},\n            'dropout': {'values': [0.2, 0.3]},\n            'cell_type': {'values': ['LSTM', 'GRU','RNN']},\n            'beam_size': {'values': [1, 3]}\n        }\n    }\n\n    # --- Run WandB Sweep --- #\n    # print(\"Starting WandB sweep...\")\n    # # Reduced count to 5 for initial testing\n\n    # sweep_id = wandb.sweep(sweep_config, project=\"DA6401-A3\")\n    # wandb.agent(sweep_id, function=sweep_train, count=25)  # Start with just 5 runs","metadata":{"_uuid":"ad03397a-9ff0-41f3-add6-9c60f8514fe8","_cell_guid":"1cedd498-1f6f-4ce1-9e74-cb4b30266077","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-20T17:51:23.712438Z","iopub.execute_input":"2025-05-20T17:51:23.712875Z","iopub.status.idle":"2025-05-20T17:51:28.389973Z","shell.execute_reply.started":"2025-05-20T17:51:23.712855Z","shell.execute_reply":"2025-05-20T17:51:28.389363Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nCUDA device count: 2\nCurrent CUDA device: 0\nCUDA device name: Tesla T4\nLoading data...\nSkipped non-string row at index 26313\nLoaded 58549 valid rows from /kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\nLoaded 5683 valid rows from /kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\nLoaded 5747 valid rows from /kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\nData loaded - Train: 58549, Dev: 5683, Test: 5747\nBuilding vocabulary...\nVocabulary size - Input: 30, Target: 67\nPreparing datasets...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"wandb.init(project=\"DA6401A3\", name=\"GRU-with-attention\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:51:58.512244Z","iopub.execute_input":"2025-05-20T17:51:58.512521Z","iopub.status.idle":"2025-05-20T17:52:05.081260Z","shell.execute_reply.started":"2025-05-20T17:51:58.512502Z","shell.execute_reply":"2025-05-20T17:52:05.080727Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_175158-vbi9jvmz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3/runs/vbi9jvmz' target=\"_blank\">GRU-with-attention</a></strong> to <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3' target=\"_blank\">https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3/runs/vbi9jvmz' target=\"_blank\">https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3/runs/vbi9jvmz</a>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3/runs/vbi9jvmz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7aebf97c8810>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\n# Load best config manually after sweep\nconfig = {\n    'emb_dim': 32,\n    'hid_dim': 64,\n    'n_layers': 1,\n    'dropout': 0.3,\n    'cell_type': 'GRU',\n    'batch_size': 64,\n    'epochs': 10,\n    'learning_rate': 0.001,\n    'teacher_forcing_ratio': 0.5\n}\n\nwandb.config.update(config)\n\n# Paths\nDATA_PATH = \"/kaggle/input/dl-agn3/dakshina_dataset_v1.0/te/lexicons\"\n\n# Read files\ntrain_df = pd.read_csv(os.path.join(DATA_PATH, \"te.translit.sampled.train.tsv\"), sep='\\t', names=[\"roman\", \"native\"])\nval_df = pd.read_csv(os.path.join(DATA_PATH, \"te.translit.sampled.dev.tsv\"), sep='\\t', names=[\"roman\", \"native\"])\ntest_df = pd.read_csv(os.path.join(DATA_PATH, \"te.translit.sampled.test.tsv\"), sep='\\t', names=[\"roman\", \"native\"])\n\ntrain_df['roman'] = train_df['roman'].astype(str)\ntrain_df['native'] = train_df['native'].astype(str)\n\ntest_df['roman'] = test_df['roman'].astype(str)\ntest_df['native'] = test_df['native'].astype(str)\n\nval_df['roman'] = val_df['roman'].astype(str)\nval_df['native'] = val_df['native'].astype(str)\n\n\n# Create character vocabularies\ndef build_vocab(sequences):\n    chars = set()\n    for seq in sequences:\n        if isinstance(seq, str):  # only process if it’s a string\n            chars.update(list(seq))\n    return ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(list(chars))\n\n\ninput_vocab = build_vocab(train_df['roman'])\noutput_vocab = build_vocab(train_df['native'])\n\n\ninput2idx = {ch: idx for idx, ch in enumerate(input_vocab)}\nidx2input = {idx: ch for ch, idx in input2idx.items()}\noutput2idx = {ch: idx for idx, ch in enumerate(output_vocab)}\nidx2output = {idx: ch for ch, idx in output2idx.items()}\n\nPAD_IDX = 0\nSOS_IDX = 1\nEOS_IDX = 2\nUNK_IDX = 3\n\n# Dataset class\nclass TransliterationDataset(Dataset):\n    def __init__(self, df):\n        self.data = df\n\n    def __len__(self):\n        return len(self.data)\n\n    def encode_seq(self, seq, vocab):\n        return [vocab.get(c, UNK_IDX) for c in seq]\n\n    def __getitem__(self, idx):\n        x = [SOS_IDX] + self.encode_seq(self.data.iloc[idx]['roman'], input2idx) + [EOS_IDX]\n        y = [SOS_IDX] + self.encode_seq(self.data.iloc[idx]['native'], output2idx) + [EOS_IDX]\n        return torch.tensor(x), torch.tensor(y)\n\ndef collate_fn(batch):\n    x_batch, y_batch = zip(*batch)\n    x_lens = [len(x) for x in x_batch]\n    y_lens = [len(y) for y in y_batch]\n    x_padded = nn.utils.rnn.pad_sequence(x_batch, padding_value=PAD_IDX)\n    y_padded = nn.utils.rnn.pad_sequence(y_batch, padding_value=PAD_IDX)\n    return x_padded, y_padded, x_lens, y_lens\n\n# DataLoaders\ntrain_loader = DataLoader(TransliterationDataset(train_df), batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(TransliterationDataset(test_df), batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n","metadata":{"_uuid":"25647966-3703-4b9c-beab-4d09a5460d7f","_cell_guid":"da850b7d-c52b-4622-9bd0-237bf9905639","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-20T17:52:07.474812Z","iopub.execute_input":"2025-05-20T17:52:07.475499Z","iopub.status.idle":"2025-05-20T17:52:07.629781Z","shell.execute_reply.started":"2025-05-20T17:52:07.475475Z","shell.execute_reply":"2025-05-20T17:52:07.629206Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=False)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src, src_len):\n        embedded = self.dropout(self.embedding(src))\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_len, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n        self.v = nn.Parameter(torch.rand(hid_dim))\n\n    def forward(self, hidden, encoder_outputs, mask):\n        seq_len = encoder_outputs.shape[0]\n        hidden = hidden.repeat(seq_len, 1, 1)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        energy = energy.permute(1, 0, 2)\n        v = self.v.repeat(encoder_outputs.size(1), 1).unsqueeze(1)\n        energy = torch.bmm(v, energy.permute(0, 2, 1)).squeeze(1)\n        energy.masked_fill_(mask == 0, -1e10)\n        return torch.softmax(energy, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n        self.attention = attention\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, encoder_outputs, mask):\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        attn_weights = self.attention(hidden, encoder_outputs, mask)\n        attn_weights = attn_weights.unsqueeze(1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        context = torch.bmm(attn_weights, encoder_outputs).permute(1, 0, 2)\n        rnn_input = torch.cat((embedded, context), dim=2)\n        output, hidden = self.rnn(rnn_input, hidden)\n        output = torch.cat((embedded.squeeze(0), output.squeeze(0), context.squeeze(0)), dim=1)\n        prediction = self.fc_out(output)\n        return prediction, hidden, attn_weights.squeeze(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_mask(src):\n    return (src != PAD_IDX).permute(1, 0)\n\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for src, trg, src_len, trg_len in tqdm(iterator):\n        src, trg = src.cuda(), trg.cuda()\n        optimizer.zero_grad()\n        mask = create_mask(src).cuda()\n        \n        encoder_outputs, hidden = model.encoder(src, src_len)\n        \n        input_char = torch.tensor([SOS_IDX]*src.size(1)).cuda()\n        outputs = torch.zeros(trg.size(0), trg.size(1), len(output_vocab)).cuda()\n        \n        for t in range(1, trg.size(0)):\n            output, hidden, _ = model.decoder(input_char, hidden, encoder_outputs, mask)\n            outputs[t] = output\n            teacher_force = random.random() < config['teacher_forcing_ratio']\n            top1 = output.argmax(1)\n            input_char = trg[t] if teacher_force else top1\n        \n        outputs_dim = outputs[1:].reshape(-1, outputs.shape[-1])\n        trg_dim = trg[1:].reshape(-1)\n        loss = criterion(outputs_dim, trg_dim)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for src, trg, src_len, trg_len in iterator:\n            src, trg = src.cuda(), trg.cuda()\n            mask = create_mask(src).cuda()\n            encoder_outputs, hidden = model.encoder(src, src_len)\n            input_char = torch.tensor([SOS_IDX]*src.size(1)).cuda()\n            outputs = torch.zeros(trg.size(0), trg.size(1), len(output_vocab)).cuda()\n            \n            for t in range(1, trg.size(0)):\n                output, hidden, _ = model.decoder(input_char, hidden, encoder_outputs, mask)\n                outputs[t] = output\n                input_char = output.argmax(1)\n                \n            outputs_dim = outputs[1:].reshape(-1, outputs.shape[-1])\n            trg_dim = trg[1:].reshape(-1)\n            loss = criterion(outputs_dim, trg_dim)\n            epoch_loss += loss.item()\n    return epoch_loss / len(iterator)\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, trg, src_len):\n        pass  # we use separate train/eval functions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attn = Attention(config['hid_dim'])\nencoder = Encoder(len(input_vocab), config['emb_dim'], config['hid_dim'], config['dropout'])\ndecoder = Decoder(len(output_vocab), config['emb_dim'], config['hid_dim'], config['dropout'], attn)\nmodel = Seq2Seq(encoder, decoder).cuda()\n\noptimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_loss = float('inf')\nfor epoch in range(config['epochs']):\n    train_loss = train(model, train_loader, optimizer, criterion, 1)\n    val_loss = evaluate(model, test_loader, criterion)\n    wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.3f}, Val Loss = {val_loss:.3f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'seq2seq_model.pth')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs('predictions_attention', exist_ok=True)\nwandb_table = wandb.Table(columns=[\"Input Roman\", \"Target Native\", \"Prediction\", \"Attention Heatmap\"])\n\ndef predict_and_save_attention(model, loader, num_samples=10):\n    model.eval()\n    all_preds, all_targets, all_inputs = [], [], []\n    with torch.no_grad():\n        samples_done = 0\n        for src, trg, src_len, trg_len in loader:\n            src, trg = src.cuda(), trg.cuda()\n            mask = create_mask(src).cuda()\n            encoder_outputs, hidden = model.encoder(src, src_len)\n            input_char = torch.tensor([SOS_IDX]).cuda()\n            decoded_indices = []\n            attentions = []\n            for _ in range(30):\n                output, hidden, attn_weights = model.decoder(input_char, hidden, encoder_outputs, mask)\n                top1 = output.argmax(1)\n                decoded_indices.append(top1.item())\n                attentions.append(attn_weights.cpu().numpy()[0])\n                input_char = top1\n                if top1.item() == EOS_IDX:\n                    break\n\n            pred = ''.join([idx2output[idx] for idx in decoded_indices if idx != EOS_IDX])\n            tgt = ''.join([idx2output[idx.item()] for idx in trg[1:] if idx.item() not in [EOS_IDX, PAD_IDX]])\n            input_roman = ''.join([idx2input[idx.item()] for idx in src[:, 0] if idx.item() not in [SOS_IDX, EOS_IDX, PAD_IDX]])\n\n            all_preds.append(pred)\n            all_targets.append(tgt)\n            all_inputs.append(input_roman)\n\n            with open(f'predictions_attention/{samples_done}.txt', 'w') as f:\n                f.write(f\"Input: {input_roman}\\nPrediction: {pred}\\nTarget: {tgt}\\n\")\n\n            if samples_done < num_samples:\n                sns.heatmap(np.array(attentions[:len(decoded_indices)]), cmap='viridis')\n                plt.title(f\"Sample {samples_done}: {pred} vs {tgt}\")\n                plt.xlabel(\"Input Characters\")\n                plt.ylabel(\"Decoder Steps\")\n                plt.savefig(f\"predictions_attention/attention_{samples_done}.png\")\n                plt.close()\n\n            wandb_table.add_data(\n                input_roman,\n                tgt,\n                pred,\n                wandb.Image(f\"predictions_attention/attention_{samples_done}.png\") if samples_done < num_samples else None\n            )\n\n            samples_done += 1\n            if samples_done == len(loader):\n                break\n\n    # Write combined predictions file\n    with open('predictions_attention/predictions.txt', 'w') as f:\n        for i, (inp, pred, tgt) in enumerate(zip(all_inputs, all_preds, all_targets)):\n            f.write(f\"Sample {i}:\\n\")\n            f.write(f\"Input: {inp}\\n\")\n            f.write(f\"Prediction: {pred}\\n\")\n            f.write(f\"Target: {tgt}\\n\\n\")\n\n    # Compute and print test accuracy\n    correct = sum([p == t for p, t in zip(all_preds, all_targets)])\n    acc = correct / len(all_preds) * 100\n    print(f\"Test Accuracy: {acc:.2f}%\")\n\n    # Log to wandb\n    wandb.log({\n        \"Test Accuracy\": acc,\n        \"Predictions Table\": wandb_table\n    })\n\n# Run it\npredict_and_save_attention(model, test_loader, num_samples=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 6","metadata":{}},{"cell_type":"code","source":"class EncoderGRU(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.gru = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout if n_layers > 1 else 0)\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_states = None  # <-- store activations\n\n    def forward(self, src, src_len):\n        embedded = self.dropout(self.embedding(src))  # [src_len, batch_size, emb_dim]\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_len, enforce_sorted=False)\n        packed_outputs, hidden = self.gru(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)  # [src_len, batch_size, hid_dim]\n\n        self.hidden_states = outputs.detach().cpu()  # Save activations\n        return outputs, hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:52:17.466791Z","iopub.execute_input":"2025-05-20T17:52:17.467057Z","iopub.status.idle":"2025-05-20T17:52:17.473589Z","shell.execute_reply.started":"2025-05-20T17:52:17.467038Z","shell.execute_reply":"2025-05-20T17:52:17.472898Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Pick a test example\nsample = test_df.iloc[0]['roman']\nprint(\"Sample input:\", sample)\n\ndef encode_sample(seq):\n    encoded = [SOS_IDX] + [input2idx.get(c, UNK_IDX) for c in seq] + [EOS_IDX]\n    return torch.tensor(encoded).unsqueeze(1)  # [seq_len, 1]\n\nsample_tensor = encode_sample(sample)\nsample_len = [sample_tensor.shape[0]]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:52:21.120397Z","iopub.execute_input":"2025-05-20T17:52:21.120670Z","iopub.status.idle":"2025-05-20T17:52:21.137502Z","shell.execute_reply.started":"2025-05-20T17:52:21.120649Z","shell.execute_reply":"2025-05-20T17:52:21.136703Z"}},"outputs":[{"name":"stdout","text":"Sample input: amkamlo\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Instantiate model (ensure you load trained weights here if available)\nencoder = EncoderGRU(input_dim=len(input_vocab),\n                     emb_dim=config['emb_dim'],\n                     hid_dim=config['hid_dim'],\n                     n_layers=config['n_layers'],\n                     dropout=config['dropout'])\n\nencoder.eval()\n\nwith torch.no_grad():\n    _ = encoder(sample_tensor, sample_len)\n    activations = encoder.hidden_states.squeeze(1)  # [seq_len, hid_dim]\n","metadata":{"_uuid":"42c6f7ef-dc58-4127-9624-57f1ccbba6ee","_cell_guid":"86b101cd-0bf2-478e-b44e-6efe009cc8fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-20T17:52:24.572835Z","iopub.execute_input":"2025-05-20T17:52:24.573651Z","iopub.status.idle":"2025-05-20T17:52:24.678536Z","shell.execute_reply.started":"2025-05-20T17:52:24.573621Z","shell.execute_reply":"2025-05-20T17:52:24.677744Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\n\n# Initialize wandb if not already\nwandb.init(project=\"transliteration-activations\")\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(activations.T, cmap=\"viridis\", cbar=True)\nplt.xlabel(\"Time Step (Input Sequence Position)\")\nplt.ylabel(\"GRU Hidden Units\")\nplt.title(f\"GRU Activations for Sample: '{sample}'\")\nplt.tight_layout()\n\n# Save locally and log to wandb\nimg_path = \"gru_activations.png\"\nplt.savefig(img_path)\nwandb.log({\"GRU Activations\": wandb.Image(img_path)})\nplt.show()\n","metadata":{"_uuid":"01c4b460-ee45-45cd-9776-84b692c74f69","_cell_guid":"b88357de-9229-4514-a024-59bcc08dbf62","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-20T17:52:28.335058Z","iopub.execute_input":"2025-05-20T17:52:28.335355Z","iopub.status.idle":"2025-05-20T17:52:35.690839Z","shell.execute_reply.started":"2025-05-20T17:52:28.335295Z","shell.execute_reply":"2025-05-20T17:52:35.690214Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">GRU-with-attention</strong> at: <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3/runs/vbi9jvmz' target=\"_blank\">https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3/runs/vbi9jvmz</a><br> View project at: <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3' target=\"_blank\">https://wandb.ai/da24m013-iit-madras-alumni-association/DA6401A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_175158-vbi9jvmz/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_175228-81l3ft04</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m013-iit-madras-alumni-association/transliteration-activations/runs/81l3ft04' target=\"_blank\">bumbling-night-1</a></strong> to <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/transliteration-activations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/transliteration-activations' target=\"_blank\">https://wandb.ai/da24m013-iit-madras-alumni-association/transliteration-activations</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m013-iit-madras-alumni-association/transliteration-activations/runs/81l3ft04' target=\"_blank\">https://wandb.ai/da24m013-iit-madras-alumni-association/transliteration-activations/runs/81l3ft04</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":10}]}